{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48b6588-b167-480c-9fe6-171df14855fe",
   "metadata": {},
   "source": [
    "## Hello DIGIT world!\n",
    "Welcome, DIGIT student Python adventurer! This is the **Jupyter Notebook** version of my [exploring-nltk.py](exploring-nltk.py) file. The cells in the notebook will help me demonstrate particular things in class without having to comment everything else out. Jupyter notebooks are written with a combination of markdown cells (like this) for documentation, and cells with executable scripts. You usually have to run the cells in order, but you can choose which ones to run. I'm splitting up my exploring-nltk.py file into tidy cells because my commenting was getting a bit out of hand inside the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383999d-5213-4f71-acca-478d58e63b1d",
   "metadata": {},
   "source": [
    "### Installs before imports \n",
    "You only need to make installations ONCE in your Python virtual environment (your venv). After that, you're good to go.\n",
    "\n",
    "To install things, open a shell (or use the terminal in PyCharm CE), and be sure the venv is activated--usually you see (venv) in parentheseses. You can activate your venv like this, after navigating to the directory above your .venv file:\n",
    "* On Mac: **.venv/bin/activate**\n",
    "* On Windows: **.venv/bin/activate.bat**\n",
    "\n",
    "We need to install nltk first: in your shell or the Pycharm terminal with .venv activated, run the following:\n",
    "\n",
    "**pip install nltk**\n",
    "\n",
    "Some of you might need to write:\n",
    "**pip3 install nltk**\n",
    "(If this is you and you're annoyed about it, make an alias for pip in your .bashrc or .zshrc to point to pip3 every time you type pip.)\n",
    "\n",
    "*If you, like me, have multiple versions of python on your machine*, run:\n",
    "\n",
    "**python3.12 -m pip install nltk**\n",
    "\n",
    "#### Other libraries to install:\n",
    "These next ones are for plotting interactive graphs:\n",
    "* **pip install matplotlib**\n",
    "* **pip install tk**\n",
    "\n",
    "#### For making Jupyter notebooks like this:\n",
    "For me to make this Jupyter notebook, I needed to install:\n",
    "* **python3.12 -m pip install notebook**\n",
    "* **python3.12 -m pip install notebook ipykernel**\n",
    "Then I just ran the following to launch the. notebook in a web browser for editing:\n",
    "**python3.12 -m jupyter notebook** or just **jupyter notebook** \n",
    "\n",
    "### After installs, time to start writing the script, with import lines\n",
    "Then we can import these things in your Python script, which is what our first executable code cell is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d477fc06-86c0-4310-b862-1108ec67b23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/eeb4/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "# The next line downloads all the example texts used in the NLTK book at https://www.nltk.org/book !\n",
    "# You can comment out the download line after the first time you do it.\n",
    "nltk.download('book')\n",
    "from nltk.book import *\n",
    "# The next line lets us do GET requests from remote URLs on the web:\n",
    "from urllib import request\n",
    "# The following import lines are for plotting interactive visualizations in Python\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e10be-393b-4c5f-b7e7-2a3f8125d36f",
   "metadata": {},
   "source": [
    "### Smoke test for graphing libraries\n",
    "After the imports, run the next cells to see if graphing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae06c6-aeba-499a-8c27-98051a9fca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116825df-d077-4ad6-ad0c-3658637a9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "### See how these words are dispersed in NLTK text 1 (Moby Dick)\n",
    "words = [\"whale\", \"sea\", \"ship\", \"captain\"]\n",
    "nltk.draw.dispersion_plot(text1, words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9306fcb-a94c-40d9-8b28-4a2560dd9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dispersion plot written closer to the NLTK example:\n",
    "# Choose the text first (text 4 is Inaugural Addresses):\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e36638-0119-4d3d-ba20-bf4e66ce5f01",
   "metadata": {},
   "source": [
    "### Take a look at some common contexts for uses of the words\n",
    "\"monstrous\" and \"very\" in text1\n",
    "Try changing these up for different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db977c6a-a18a-4ed2-9eb0-f25e71fc1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.common_contexts([\"find\",\"seek\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9472b-89df-4070-9045-c04af7748ccb",
   "metadata": {},
   "source": [
    "### Look for similar words\n",
    "This looks for words that appear in the same context as the word you enter\n",
    "SO, I used text6 (\"Monty Python and the Holy Grail\") for this next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f0c62-9a74-41f9-b577-a4f6b81dec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.similar('grail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465740a-884d-44db-92bf-da743d40bf9e",
   "metadata": {},
   "source": [
    "### Lexical Diversity\n",
    "Evaluate this simple but profound metric with a count of the DISTINCT words divided by a count of all the words.\n",
    "We'll write a simple **function** for this, following the NLTK book. Notice how a function uses special syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70d5a97-4ffa-4627-8c63-62f233de8444",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (2085371568.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"mobyDick\" = {mobyDick}\")\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) * 100 \n",
    "    # ebb: I'm just  multipling by 100 to yield this as a percentage value\n",
    "\n",
    "montyPython = lexical_diversity(text6)  \n",
    "mobyDick = lexical_diversity(text3)\n",
    "\n",
    "print(f\"montyPython = {montyPython}\")\n",
    "print(f\"mobyDick = {mobyDick}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7b895-cd86-43c2-8a84-e4c3a356c0e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pulling files from a web url\n",
    "For this, let's pull *Blithedale Romance* direct from Project Gutenberg (just like David did with it while introducing invisible XML).\n",
    "I'm also demonstrating **how to make \"picture string\" variables** so you can easily know what you're printing out in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8adfbb-d331-4f9a-a3f3-bb979ebab025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blithedale Romance text file on Project Gutenberg\n",
    "bookurl= \"https://www.gutenberg.org/cache/epub/2081/pg2081.txt\"\n",
    "response = request.urlopen(bookurl)\n",
    "br = response.read().decode('utf8')\n",
    "type(br)\n",
    "print(len(br))\n",
    "# make a variable\n",
    "howLong = len(br)\n",
    "# picture string version! \n",
    "print(f\"howLong = {howLong}\")\n",
    "novelSlice = br[:500]\n",
    "print(f\"novelSlice = {novelSlice}\")\n",
    "\n",
    "# splitEmUp = br.split()\n",
    "# print(f\"splitEmUp = {splitEmUp}\")\n",
    "\n",
    "for token in splitEmUp:\n",
    "    if token.endswith('ing'):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af8a0f-c093-4035-b60c-11f9e45db9ef",
   "metadata": {},
   "source": [
    "### Make a Text Concordance \n",
    "Use the concordance feature...In the NLTK book, they introduce this with the prefab text of Jane Austen's Emma (already in NLTK's text corpora). I bet we can do this with our split-up Blithedale text that we pulled in from Project Gutenberg...\n",
    "**NOTE**: You need to execute the previous cell for the next one to know the variables it needs.\n",
    "\n",
    "Basically, to make the concordance, you have to convert the list of tokens into a special NLTK **text object**, and then run the concordance feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1b4aa-ec10-455a-b94d-7955ea3014dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance = nltk.Text(splitEmUp).concordance(\"living\")\n",
    "print(f\"concordance = {concordance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bdaea-db71-4319-b38e-e46af4672a01",
   "metadata": {},
   "source": [
    "### Frequency Distributions\n",
    "Here's an example plotting of frequency distributions. This is from the NLTK book, and you might be wondering why they didn't just use text4 for the corpus--which has all the addresses baked together in one file. They pulled from a **different** set, a collection of texts with each address stored in just one file, because the year of each address is in the fileid property! Being "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e55d8a-7a26-40b9-a1cb-d6e44d02e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target,fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    # I added the next line to help make the plot more legible!\n",
    "    if int(fileid[:4]) > 1990\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
