{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38a5c62-1925-4c1e-b36b-16cd8d2fcf27",
   "metadata": {},
   "source": [
    "## Words to Network Data\n",
    "\n",
    "Now that we know some ways to pull an interesting selection of words, let's think about them like a network. Network analysis involves looking at how things relate to each other based on shared properties. \n",
    "\n",
    "In this notebook, we will address a simple research question: Which **files** (or **chapters**, or **scenes**, or **authors**, or **titles**) share each others' top 5 or 10 **verbs** (or **adjectives**, or **nouns**, or **WordNet synsets**, etc)? \n",
    "\n",
    "We will try exploring this as a network of data and import it into network visualization software. \n",
    "\n",
    "### Installs in your Terminal \n",
    "* If you did not do this in the previous notebook: `pip install spacy` (or `python3.12 -m pip install spacy`)\n",
    "* We need the **pandas** library to make dataframes: `pip install pandas` (or `python3.12 -m pip install pandas`)\n",
    "\n",
    "<pre>\n",
    "    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣶⣿⣷⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⣦⡀\n",
    "⠀⠀⠀⠀⢀⠄⠀⠀⠀⠀⢈⣿⣿⣿⠟⠛⠁⠀⠀⠀⠀⠀⠀⠐⢿⣿⣿⣿⣿⣷\n",
    "⠀⠀⢀⡔⠁⠀⠀⠀⢀⣴⡿⠃⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⠟⠁\n",
    "⠀⣠⡟⠀⠀⠀⠀⣰⣿⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⢠⣿⠁⠀⠀⠀⢰⣿⣿⡏⠀⠀⠀⢀⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣾⣿⠀⠀⠀⠀⢸⣿⣿⡇⠀⠀⢠⣿⣿⣿⣿⠀⠀⠀⠀⣰⣾⣶⣆⠀⠀⠀⠀⡄\n",
    "⣿⣿⡀⠀⠀⠀⢸⣿⣿⣧⠀⠀⢸⣿⣿⡿⠃⠀⠀⠀⠀⢿⣿⣿⣿⡆⠀⠀⢸⣧\n",
    "⢿⣿⣧⠀⠀⠀⢸⣿⣿⣿⣦⡀⠀⠉⠉⠀⠀⠀⠀⠀⠀⠈⠻⣿⠿⠁⠀⢠⣿⣿\n",
    "⢸⣿⣿⣷⣄⠀⣿⣿⣿⣿⣿⣿⣷⣦⣄⠀⠲⣶⣶⣶⠀⠀⠀⠀⠀⣀⣴⣿⣿⣿\n",
    "⢰⣿⣿⣿⣿⣷⣽⣿⣿⣿⣿⣿⣿⣿⣿⣦⡄⣬⣅⣀⣠⣾⣿⣿⣿⣿⣿⣿⣿⡟\n",
    "⠈⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣤⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇\n",
    "⠀⠹⣿⣿⣿⣿⣿⣿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡏⠀⠈⣿⣿⣿⣿⣿⣿⣿⡿⠀\n",
    "⠀⠀⠻⣿⣿⣿⣿⣿⡎⢿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⣿⣿⣿⣿⣿⣿⣿⠃⠀\n",
    "⠀⠀⠀⠙⠻⠿⣿⣿⠗⠀⢻⣿⣿⣿⣿⣿⣿⣿⡇⠀⢸⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⣿⣿⣿⣿⣿⣿⣿⠀⢸⣿⣿⣿⡿⠟⠁⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠿⢿⣿⣿⡿⠇⠀⠈⠉⠁⠀⠀⠀⠀⠀⠀\n",
    "</pre>\n",
    "\n",
    "\n",
    "\n",
    "Image credit: <https://emojicombos.com/panda-ascii-art> \n",
    "\n",
    "### Language Models\n",
    "Like NLTK's collections that we downloaded, [spaCy has trained language models](https://spacy.io/usage/models) to download. You download these in your Python script after you import spacy, and after you download once, you don't need to do it again (so you can comment out the download line). We're going to try the medium and large models in English. (It's good to know that both spaCy and NLTK have resources for NLP on other languages, too!)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863583d-944b-4cdb-8329-8f89cbb9c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b96b7-90fa-49cb-bd8f-31c79e327de5",
   "metadata": {},
   "source": [
    "### Downloading language models\n",
    "To work with spaCy's pre-trained language models, you need to download them to you virtual environment. There are:\n",
    "* en_core_web_sm (smallest--not as much info as the others)\n",
    "* **en_core_web_md** (Pretty good date here, size: 34 MB )\n",
    "* **en_core_web_lg** (Lots of data here, size: 400 MB.)\n",
    "Note that the LARGEST  one will have the most data and probably be the most reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f65be-d71c-4e01-85ae-12f5aa8918be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN YOU SKIP THIS???\n",
    "# After you download a model into your virtual environment for the first time, you can comment out the download line.\n",
    "# spaCy's medium and large models will give us the best results for NLP tagging.\n",
    "# nlp = spacy.cli.download(\"en_core_web_sm\")\n",
    "# nlp = spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e169c-e7d3-43db-a4ce-e60d9397d1cd",
   "metadata": {},
   "source": [
    "### Load the model \n",
    "Now we redefine the nlp variable to LOAD the model you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d584c8-744d-48ba-90c8-d4bfef5931f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abeffa-a806-46db-803c-b8595d0676d6",
   "metadata": {},
   "source": [
    "## File Collection\n",
    "You can set this to read a collection of text files that you've prepped for natural language processing. \n",
    "For this notebook, we'll work with just the dialogue text we pulled from the One Piece project. \n",
    "The text is organized as one file for each volume of OnePiece, so we can maybe explore how much the dialogue is relying on certain words, and how it might be changing over the volumes.\n",
    "\n",
    "We're going to get volume information from OnePiece's filenames (which look like this: `vol-9.txt'. We can use the `os` library to help us isolate the filename from the file extension. Let's try that so we can simplify our references to the volumes when we output our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd15c8-4a46-44fd-9b05-6019b86d7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "collPath = 'onepiece-nlp-text'\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        print(name)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            lengthFile = len(readFile)\n",
    "            print(lengthFile)\n",
    "# We're just printing out the filepaths and lengths of the files as a \"smoke test\" to see if we're reading the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e08af-56b1-45c4-8d80-1b78f7c7290f",
   "metadata": {},
   "source": [
    "## Collect some words with a little help from spaCy\n",
    "Let's build this up to go file by file. We'll start by surveying what we have...(Ultimately we want to collect the top 5 or 10 most frequently repeating words of a kind.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea393d-50f2-4f42-9096-a4ca16826cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SURVEY IT ALL! :-) (Yeah, lots of data...)\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        print(name)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            spacyRead = nlp(readFile)\n",
    "            for token in spacyRead:\n",
    "                print(token.text, \"---->\", token.pos_, \":::::\", token.lemma_)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb72cf-2259-4821-b4c8-d5754fc195ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, we can request an explanation from spaCy of its tags: \n",
    "spacy.explain(\"DET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b71a0-8653-43ae-b767-4f738aa0a08e",
   "metadata": {},
   "source": [
    "### Okay, let's select something interesting\n",
    " ...and maybe only collect the lemmas.\n",
    "\n",
    "Let's do this with a **function** call. We can easily switch the kind of word we want to output this way.\n",
    "Here we are starting to associate words wtih volumes...we're going to refine that by making a dataframe next.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b154915-9ae0-4f06-a178-fe0ad8cef0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wordCollector(words, unit):\n",
    "    wordList = []\n",
    "    for token in words:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            wordList.append((token.lemma_, unit))\n",
    "    uniqueLems = set(wordList)\n",
    "    return uniqueLems\n",
    "\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            spacyRead = nlp(readFile)\n",
    "            myWords = wordCollector(spacyRead, name)\n",
    "            print(myWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064f667-1cad-4a97-8619-b8e04e050464",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Let's try dataframes to store these relations of word-form to unit \n",
    "Dataframes, supporteed by the pandas library in Python, facilitate handling data in spreadsheet / CSV / TSV format.\n",
    "We want to output some data to a TSV (tab-separated values file) so we can send it to network visualization software.\n",
    "\n",
    "We'll need to open a terminal and  **pip install pandas** (or python3.12 -m pip install pandas) to get dataframes working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c54ce9-69b0-4950-989b-f109d79a91f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def wordCollector(words, unit):\n",
    "    wordList = []\n",
    "    nodeAtts = []\n",
    "    unitList = []\n",
    "    for token in words:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            wordList.append(token.lemma_)\n",
    "            nodeAtts.append(token.pos_)\n",
    "            unitList.append(unit)\n",
    "\n",
    "    data = {\n",
    "        'word': wordList,\n",
    "        'nodeType': nodeAtts,\n",
    "        'unit': unitList\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            spacyRead = nlp(readFile)\n",
    "            myDataFrame = wordCollector(spacyRead, name)\n",
    "            print(myDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28c039-9a52-4d5a-89c0-1ca5fd7c91b4",
   "metadata": {},
   "source": [
    "## Write the dataframes to an output TSV file\n",
    "TSV means **tab-separated-values**. We'll use the tab character as a separator.\n",
    "The other popular alternative is the CSV: **comma-separated-values**, which uses the comma as a separator.\n",
    "You can use either one for our network data.\n",
    "\n",
    "I'm going to output some \"node attributes\" for tne network. In a network visualization, we'll want to be able to quickly distinguish the word nodes from the unit nodes (possibly by shape), and providing a qualifier or \"node attribute\" in the dataframe will be helpful.\n",
    "We can add other kinds of node attributes perhaps as numerical ranges (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7a42f-14e2-4a73-9d5b-70e04f5d9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def wordCollector(words, unit):\n",
    "    wordList = []\n",
    "    nodeAtts = []\n",
    "    unitList = []\n",
    "    for token in words:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            wordList.append(token.lemma_)\n",
    "            nodeAtts.append(token.pos_)\n",
    "            unitList.append(unit)\n",
    "\n",
    "    data = {\n",
    "        'word': wordList,\n",
    "        'nodeType': nodeAtts,\n",
    "        'unit': unitList\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "    # This is returning a separate dataframe for every source text file. \n",
    "\n",
    "# We need to consolidate all the dataframes into one file. Collect all dataframes here!\n",
    "allDataFrames = []\n",
    "\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            spacyRead = nlp(readFile)\n",
    "            myDataFrame = wordCollector(spacyRead, name)\n",
    "            # Add each individual dataframe as it comes out into the list of dataframes!\n",
    "            allDataFrames.append(myDataFrame)\n",
    "\n",
    "# Make an output filepath\n",
    "outputFilePath = 'networkData.tsv'\n",
    "# Turn the list of dataframes into one dataframe:\n",
    "fullDataFrame = pd.concat(allDataFrames, ignore_index=True)\n",
    "\n",
    "# Note, since Pandas knows how to open and write files line by line, we can skip that open() step we used last time.\n",
    "fullDataFrame.to_csv(outputFilePath, sep='\\t', index=False)\n",
    "print('I just saved a dataframe as a TSV file.')\n",
    "# Go check your filestash for the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022827ee-8bcb-42f5-bce2-bd85c9bc3c5b",
   "metadata": {},
   "source": [
    "### Something to try...\n",
    "* Try developing this to return synsets from WordNet.\n",
    "* Add an Ambiguity metric to count the number of synsets it belongs to.\n",
    "* Output this as a second \"node attribute\" qualifier to the Part of Speech. We could use that to color-code the words by ambiguity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159d435-7f91-453d-9e28-32e1af0d4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, with Wordnet data!\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def wordCollector(words, unit):\n",
    "    wordList = []\n",
    "    nodeAtts = []\n",
    "    synsetCounts = []\n",
    "    unitList = []\n",
    "    for token in words:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            synsets = len(wn.synsets(token.lemma_))\n",
    "            # We should be able to look up and count those synsets right here!          \n",
    "            wordList.append(token.lemma_)\n",
    "            nodeAtts.append(token.pos_)\n",
    "            synsetCounts.append(synsets)\n",
    "            unitList.append(unit)\n",
    "                          \n",
    "    data = {\n",
    "        'word': wordList,\n",
    "        'nodeType': nodeAtts,\n",
    "        'synsetCount': synsetCounts,\n",
    "        'unit': unitList\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "    # This is returning a separate dataframe for every source text file. \n",
    "\n",
    "# We need to consolidate all the dataframes into one file. Collect all dataframes here!\n",
    "allDataFrames = []\n",
    "\n",
    "for file in os.listdir(collPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collPath}/{file}\"\n",
    "        name, extension = os.path.splitext(file)\n",
    "        with open(filepath, 'r', encoding='utf8') as f:\n",
    "            readFile = f.read()\n",
    "            spacyRead = nlp(readFile)\n",
    "            myDataFrame = wordCollector(spacyRead, name)\n",
    "            # Add each individual dataframe as it comes out into the list of dataframes!\n",
    "            allDataFrames.append(myDataFrame)\n",
    "\n",
    "# Make an output filepath\n",
    "outputFilePath = 'networkData-2.tsv'\n",
    "# Turn the list of dataframes into one dataframe:\n",
    "fullDataFrame = pd.concat(allDataFrames, ignore_index=True)\n",
    "\n",
    "# Note, since Pandas knows how to open and write files line by line, we can skip that open() step we used last time.\n",
    "fullDataFrame.to_csv(outputFilePath, sep='\\t', index=False)\n",
    "print('I just saved a NEW dataframe as a NEW TSV file.')\n",
    "# Go check your filestash for the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc30c0-9813-4045-b983-5a272e02a8f3",
   "metadata": {},
   "source": [
    "## What will we do with this fancy TSV?\n",
    "\n",
    "We can analyze it as a **network** of data using network analysis. You could try importing your TSV into kumu.io, but we're going to introduce Cytoscape, which is a very powerful, artful method with lots of visualization options. \n",
    "\n",
    "To view the data we extracted in this notebook as an SVG export from Cytoscape, see <https://raw.githubusercontent.com/newtfire/textAnalysis-Hub/refs/heads/main/Class-Examples/Python/readFileCollections-examples/networkData-2.svg>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf362bdd-2da4-4107-ade3-5f310c5be43f",
   "metadata": {},
   "source": [
    "## Your Turn! \n",
    "Adapt this notebook to explore one or more of your project files and prepare some network data of your own. \n",
    "\n",
    "* Output a CSV or TSV of data from your project for import into our network visualization software.\n",
    "* [Install Cytoscape](https://cytoscape.org/) on your computer and check to make sure it is working.\n",
    "     * Cytoscape now comes bundled with its own Java, so you should be good to go for running it (much like oXygen). If you have trouble, work through the [Troubleshooting](https://cytoscape.org/troubleshooting.html) guide first. Bring questions to class or via Canvas!\n",
    "     * After you install Cytoscape, see if you can also install the yFiles Layout Algorithms. Open Cytoscape, go to **Layout → Install yFiles**, which directs you to an App store to download it and apply it to Cytoscape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d44d-f259-4e1a-8391-8b4702c9485e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
