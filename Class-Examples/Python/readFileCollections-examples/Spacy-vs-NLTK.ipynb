{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38a5c62-1925-4c1e-b36b-16cd8d2fcf27",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging is Weird\n",
    "\n",
    "NLTK shows us some weird part of speech tagging. We might decide that it's just a bit unreliable here. In this notebook, we're going to compare the part of speech tagging from NLTK with a newer library called spaCy that's used widely in industry for natural language processing (NLP). We'll also practice preparing some simple files for output. \n",
    "\n",
    "### Installs\n",
    "To try out spaCy, you need to **open your terminal and install it first**:\n",
    "`pip install spacy` \n",
    "\n",
    "### Language Models\n",
    "Like NLTK's collections that we downloaded, [spaCy has trained language models](https://spacy.io/usage/models) to download. You download these in your Python script after you import spacy, and after you download once, you don't need to do it again (so you can comment out the download line). We're going to try the medium and large models in English. (It's good to know that both spaCy and NLTK have resources for NLP on other languages, too!)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863583d-944b-4cdb-8329-8f89cbb9c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b96b7-90fa-49cb-bd8f-31c79e327de5",
   "metadata": {},
   "source": [
    "### Downloading language models\n",
    "To work with spaCy's pre-trained language models, you need to download them to you virtual environment. There are:\n",
    "* en_core_web_sm (smallest--not as much info as the others)\n",
    "* **en_core_web_md** (Pretty good date here, size: 34 MB )\n",
    "* **en_core_web_lg** (Lots of data here, size: 400 MB.)\n",
    "Note that the LARGEST  one will have the most data and probably be the most reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f65be-d71c-4e01-85ae-12f5aa8918be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN YOU SKIP THIS???\n",
    "# After you download a model into your virtual environment for the first time, you can comment out the download line.\n",
    "# spaCy's medium and large models will give us the best results for NLP tagging.\n",
    "# nlp = spacy.cli.download(\"en_core_web_sm\")\n",
    "# nlp = spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e169c-e7d3-43db-a4ce-e60d9397d1cd",
   "metadata": {},
   "source": [
    "### Load the model \n",
    "Now we redefine the nlp variable to LOAD the model you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b994b-63fa-4143-b460-186f8b123aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd15c8-4a46-44fd-9b05-6019b86d7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'hughes-txt/sixteen.txt'\n",
    "f = open(filepath, 'r', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e08af-56b1-45c4-8d80-1b78f7c7290f",
   "metadata": {},
   "source": [
    "## spaCy Part of Speech Tagging\n",
    "spaCy needs to read the str() of our text to generate a dictionary of information on each word.\n",
    "So we go back to our opened file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33ad03-023f-4344-bf3c-913e9e74d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used nlp as our variable for the spaCy operations. \n",
    "# f is our variable for the source file. spaCy doesn't tell you how it tokenizes or what it's doing (sigh). \n",
    "spacyRead = nlp(f)\n",
    "for token in spacyRead:\n",
    "    print(token.text, \"---->\", token.pos_, \":::::\", token.lemma_)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfb906-3ef6-454c-8318-5467bc51aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"VERB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a523193-8091-43da-9eaf-c94c1bd527dc",
   "metadata": {},
   "source": [
    "### For spaCy: define a function to collect the words you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d43a3-3597-4ab3-ab0a-b619468dbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCollector(words):\n",
    "    wordList = []\n",
    "    count = 0\n",
    "    for token in words:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            count += 1\n",
    "            # print(count, \": \", token.text, \" lemma: \", token.lemma_, \" pos: \", token.pos_)\n",
    "            # don't forget the underscore after token.lemma_ , token.pos_, etc.!\n",
    "            wordList.append(token.lemma_)\n",
    "            # print(count, \": \", token, token.pos_)\n",
    "    # print(count, \": \", adjectives)\n",
    "    return wordList\n",
    "myWords = wordCollector(spacyRead)\n",
    "print(myWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c819d4-9ad7-4188-aaeb-5bb8315874f9",
   "metadata": {},
   "source": [
    "### Frequency of words\n",
    "Here is something we can do: Because we didn't make a set of unique words, we have a list full of the original words. \n",
    "The Counter function in collections offers a speedy way to count the number of times something appears in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991948d9-8a36-473d-89f5-69fe77e792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(myWords)\n",
    "# most_common() converts the Counter's dictionary to a tuple series and sorts it\n",
    "ranked = word_freq.most_common()\n",
    "topTen = word_freq.most_common(10)\n",
    "print(topTen)\n",
    "lastTen = word_freq.most_common()[:-11:-1]\n",
    "print(lastTen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28c039-9a52-4d5a-89c0-1ca5fd7c91b4",
   "metadata": {},
   "source": [
    "## Write something to an output file (just text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416e4dd-6fe9-4614-9e18-9daccccf09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = open(\"verbFreq.txt\", \"w\")\n",
    "for r in ranked:\n",
    "    o.write(str(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df401a-a223-4342-8162-d8a3863c167a",
   "metadata": {},
   "source": [
    "## Plotting a simple chart with a Python Library\n",
    "\n",
    "We have data we can use to plot. There are SVG plotting libraries designed for Python, and one of the best for us working on websites is PyGal, since it's easy to customize and outputs a nice file for use on a website. \n",
    "\n",
    "Read the [PyGal documentation](https://www.pygal.org/en/stable/documentation/index.html) (or any plotting library you find) to see how you can adapt it as you wish. \n",
    "\n",
    "We will need to open a terminal and **pip install pygal** or **python3.12 -m pip install pygal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55a77c-b36f-46c1-929f-c7499c98372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygal\n",
    "from pygal.style import DarkSolarizedStyle\n",
    "\n",
    "# There are lots of chart types in pygal. Bar is a nice simple one:\n",
    "bar_chart = pygal.Bar(style=DarkSolarizedStyle)\n",
    "bar_chart.title = 'Top 10 Most Frequent Verbs in Sixteen Candles'\n",
    "\n",
    "# Add data to the chart\n",
    "for word, freq in topTen:\n",
    "    bar_chart.add(word, freq)\n",
    "\n",
    "# Render to file (SVG format)\n",
    "bar_chart.render_to_file('top10_verbs.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f570e-6bda-4f12-b61c-ecab31436759",
   "metadata": {},
   "source": [
    "### Send my list of words OR word lemmas to WordNet and get me synset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26d00e-67a0-450c-9ce4-07a4ac454087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8845e-d637-43f8-8989-5be7ae035a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "setOfWords = set(myWords)\n",
    "lowerCased = [w.lower() for w in setOfWords]\n",
    "\n",
    "sortedWords = sorted(lowerCased)\n",
    "print(sortedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d076dc-791b-4376-adec-d3619e8c0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in myWords:\n",
    "    synsets = len(wn.synsets(w))\n",
    "    print(\"The word \", w, \"belongs to \", synsets, \"synsets in WordNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26e21b-aef7-46c4-945b-33727cfb2575",
   "metadata": {},
   "source": [
    "## PyGal Fun with Ambiguity Data\n",
    "Okay, what if I want to customize the colors of my plot using the synset ambiguity information? \n",
    "This is a little tricky, but we can work with the rgb() color values to plot on a scale of 0 to 255. Here's a little recipe I cooked up, and I got a little help from ChatGPT for the color function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37912068-0888-4c83-a794-234cc2d12f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pygal\n",
    "from pygal.style import Style\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_freq = Counter(myWords)\n",
    "topTen = word_freq.most_common(10)\n",
    "print(topTen)\n",
    "\n",
    "# Step 1: Get synset count for each of the top 10 verbs\n",
    "synset_counts = {}\n",
    "for word, freq in topTen:\n",
    "    synset_counts[word] = len(wn.synsets(word, pos='v'))\n",
    "\n",
    "# Step 2: Normalize synset counts to 0â€“255 for coloring\n",
    "max_syn = max(synset_counts.values())\n",
    "min_syn = min(synset_counts.values())\n",
    "\n",
    "def synset_to_color(syn_count):\n",
    "    \"\"\"Maps synset count to a shade of blue (darker = more ambiguous)\"\"\"\n",
    "    if max_syn == min_syn:\n",
    "        blue = 150\n",
    "    else:\n",
    "        blue = int(50 + 205 * (syn_count - min_syn) / (max_syn - min_syn))\n",
    "    return f'#{0:02x}{0:02x}{blue:02x}'\n",
    "\n",
    "# Step 3: Create custom Pygal style\n",
    "custom_style = Style(\n",
    "    background='white',\n",
    "    plot_background='white',\n",
    "    foreground='black',\n",
    "    foreground_strong='black',\n",
    "    foreground_subtle='gray',\n",
    "    opacity='.8',\n",
    "    opacity_hover='.9',\n",
    "    transition='400ms ease-in',\n",
    "    colors=('#cccccc',)  # Give PyGal a dummy color to start\n",
    ")\n",
    "\n",
    "# Step 4: Create the bar chart\n",
    "bar_chart = pygal.Bar(style=custom_style)\n",
    "bar_chart.title = 'Top 10 Verb Frequencies Colored by Ambiguity (Synsets)'\n",
    "\n",
    "for word, freq in topTen:\n",
    "    syns = synset_counts[word]\n",
    "    color = synset_to_color(syns)\n",
    "    bar_chart.add(word, [{'value': freq, 'label': f'{word} has {syns} synsets', 'style': f'fill:{color}'}])\n",
    "    # I can add whatever new info I want (beyond the 'value') to PyGal's tooltip using 'label': \n",
    "\n",
    "# Step 5: Render the SVG chart\n",
    "bar_chart.render_to_file('top_verbs_colored_by_synsets.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe44f9-95c3-4067-abef-e85a28f0bcca",
   "metadata": {},
   "source": [
    "## Your Turn! \n",
    "Adapt this notebook to explore one or more of your project files. Be sure that you are able to:\n",
    "* **Open a file with Python first before you tokenize it**. (This time, try out spaCy.)\n",
    "* Collect a selection of words, with POS tagging or without it. (Can you output all the tokens that just have the same beginning or ending?)\n",
    "* Options:\n",
    "     * Send your words or word lemmas to WordNet and return synset info, and/or\n",
    "     * Find out about how frequently each of your selected tokens is used in the text using the Counter.\n",
    "* This time, **open a file to write some of the information you retrieve to output**. \n",
    "* Add, commit, and push to your GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4362179-0c21-4968-8fc7-27a05fd7bd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
