{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78f72ba-82c5-4033-8b74-fdc0e9f42290",
   "metadata": {},
   "source": [
    "# Words by Numbers: Exploring Vector Similarity \n",
    "This notebook accompanies our readings on AI, to help explore words of interest as \"vector embeddings\". As we are learning from our readings, in the making of a large language model, they consist of calculations about meaningful units of language (words / tokenized particles) and they calculate relationships based on mathematical vectors. \n",
    "\n",
    "In this digital humanities orientation to language models and word embeddings, we'll reach for the number values (\"vector word embedding values\") that language models apply to word-tokens when they read and generate them.\n",
    "\n",
    "*My first version of this notebook is [this Python file](https://github.com/newtfire/textAnalysis-Hub/blob/main/Class-Examples/Python/readFileCollections-examples/readingFileCollection.py) also stored in our textAnalysis-Hub Python collection.\n",
    "\n",
    "## Pay attention to an interesting word: A little lab test of LLMs\n",
    "We'll just do this as a test case based on some text file outputs from any generative AI. (You can also adapt this script to explore your own text corpora for your projects.) For our demonstration, we'll run a little experiment on generative AI chatbots (ChatGPT, Claude, Gemini, etc). We can give a couple of these bots the same prompt, and save the outputs in a directory. Then we'll run this notebook over them.\n",
    "\n",
    "When you review outputs to your prompts, do you see a **word or idea** that's sometimes repeated or that seems an interesting basis for comparison across the files? Take your cue from this: Choose a word of interest that you think is worth \"paying attention to\" as a basis for comparing the text outputs of the AI (or the texts in your collection). In this notebook, we'll use it as a basis for comparing all the files: **how often do they use a similar word**, that is, a \"similar\" word based on a language model's calculations of similarity? \n",
    "\n",
    "For our experiment, we'll work with spaCy's freely available large language model, to access its vector database on words and return calculations. Python can \"crunch the numbers\" to show us which prompts are using words that are mapped to a nearby \"vector space\" in spaCy's language model. This Python script can help show us how our texts compare to each other based on how often they draw from words that share similar number values. \n",
    "\n",
    "For this experiment we'll be accessing the **cosine similarity** values that LLMs reach for in calculating a response to a prompt. The only difference is, we're using spaCy's embedding dictionary as our basis for exploring. (Our little experiment is simply using spaCy's vector embeddings as our similarity measuring instrument. To take this further, we could try to get \"under the hood\" of each LLM to access its distinctive vector database.)\n",
    "\n",
    "## Installs and imports\n",
    "* spacy\n",
    "* spacy's large language model\n",
    "* os for handling filepaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33867d30-f8db-4308-8852-e7864541955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# COMMENT OUT THE spacy.load line after the first time you load your model!\n",
    "# If the large one is too big, you can use the medium:\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "# en_core_web_md: size: 34 MB )\n",
    "# en_core_web_lg (Lots of data here, size: 400 MB.) Note that the LARGEST one will have the most data and probably be the most reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29145315-13c7-49f7-b8f6-09a8a3ba60e6",
   "metadata": {},
   "source": [
    "### Identify the directory to read\n",
    "So, you have a collection of text files that you want to compare, and you've stored them in a directory(folder). \n",
    "Which directory do you want the Python script to read? Define it in relation to where you saved your Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da21b8-78f3-4bc5-b39c-322398901b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'textCollection'\n",
    "# This is a folder saved in the same directory with this Python notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ab633-c6ea-468d-84d1-20312d7a1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Smoke test\": Open the directory and make sure we can access the files inside. \n",
    "# Here's where we use os.\n",
    "def readTextFiles(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "    # ebb: add that utf8 encoding argument to the open() function to ensure that reading works on everyone's systems\n",
    "    # this all succeeds if you see the text of your files printed in the console.\n",
    "        readFile = f.read()\n",
    "        # print(readFile)\n",
    "        stringFile = str(readFile)\n",
    "        lengthFile = len(readFile)\n",
    "        # (Literally, we'll just count the number of characters in this.)\n",
    "        print(lengthFile)\n",
    "\n",
    "for file in os.listdir(collection):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collection}/{file}\"\n",
    "        print(filepath)\n",
    "        readTextFiles(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e7381-ad94-4d0e-b88a-6401fd4d611f",
   "metadata": {},
   "source": [
    "### Apply spaCy's language model \n",
    "\n",
    "* First we'll have spaCy tokenize and \"read\" the files, just using its default tokenizer.\n",
    "* Then we'll choose our **word of interest** that we'll use as a basis for exploring similarity.\n",
    "* We'll then create a **dictionary of highly similar values** to that word, drawn from spaCy's vector database.\n",
    "\n",
    "The similarity value is called \"cosine similarity\" and it varies between 0 and 1, with 1 being closest to identical.\n",
    "Try adjusting the \"similarity gage\" by tweaking the number in the second if statement: \n",
    "\n",
    "```py\n",
    "            if wordOfInterest.similarity(token) > .3:\n",
    "```\n",
    "\n",
    "How does changing this value affect the results you see?\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738acac7-9fbf-4fd3-a40b-89a6fb4bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTextFiles(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        readFile = f.read()\n",
    "        # print(readFile)\n",
    "        stringFile = str(readFile)\n",
    "\n",
    "        tokens = nlp(stringFile)\n",
    "        # playing with vectors here\n",
    "        vectors = tokens.vector\n",
    "        # Want to \"see\" some vector information? Uncomment the  next line:\n",
    "        # print(vectors)\n",
    "\n",
    "        wordOfInterest = nlp(u'panic')\n",
    "\n",
    "         # Now, let's open an empty dictionary! We'll fill it up with the for loop just after it.\n",
    "        # The for-loop goes over each token and gets its values\n",
    "        highSimilarityDict = {}\n",
    "        for token in tokens:\n",
    "            if(token and token.vector_norm):\n",
    "                if wordOfInterest.similarity(token) > .4:\n",
    "                # ^^^^ our \"similarity gage\" ^^^^\n",
    "                    highSimilarityDict[token] = wordOfInterest.similarity(token)\n",
    "                    # The line above creates the structure for each entry in my dictionary.\n",
    "                        # print(token.text, \"about this much similar to\", wordOfInterest, \": \", wordOfInterest.similarity(token))\n",
    "        print(f'This is a dictionary of words most similar to the word \"{wordOfInterest.text}\" in \"{file}\".')\n",
    "        print(highSimilarityDict)\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "\n",
    "for file in os.listdir(collection):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collection}/{file}\"\n",
    "        readTextFiles(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30a13e-5f8b-4dd5-8551-4cc89ea34d9f",
   "metadata": {},
   "source": [
    "#### \"Dedupe\" and sort the dictionary\n",
    "The results above give you duplicate words based on how often they appear in the text. That's information we might want to count, but still just have the word appear once. We might also want to sort the values from most to least similar to our word of interest.\n",
    "\n",
    "So we'll try to do these things in the next cell. The **Counter** function from the collections library will be super helpful here because it takes distinct values AND counts how often a value appears in the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55395b52-64b5-4e29-80b9-2279a6f7dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def readTextFiles(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        readFile = f.read()\n",
    "        # print(readFile)\n",
    "        stringFile = str(readFile)\n",
    "\n",
    "        tokens = nlp(stringFile)\n",
    "        # playing with vectors here\n",
    "        vectors = tokens.vector\n",
    "        # Want to \"see\" some vector information? Uncomment the  next line:\n",
    "        # print(vectors)\n",
    "\n",
    "        wordOfInterest = nlp(u'panic')\n",
    "\n",
    "        # Our structures for storing similarity data:\n",
    "        # This time we'll add counts, \n",
    "        # and we'll use Python's sorted() function to sort by similarity values from high (close to 1) to low (close to 0):\n",
    "        highSimilarityDict = {}\n",
    "        sorted_similarity = sorted(highSimilarityDict.items(), key=lambda item: item[1], reverse=True)\n",
    "        wordCounts = Counter()\n",
    "        \n",
    "        for token in tokens:\n",
    "            if(token and token.vector_norm):\n",
    "                if wordOfInterest.similarity(token) > .4:\n",
    "                    wordCounts[token.text] += 1\n",
    "                # ^^^^ our \"similarity gage\" ^^^^\n",
    "                    highSimilarityDict[token] = wordOfInterest.similarity(token)\n",
    "                    # The line above creates the structure for each entry in my dictionary.\n",
    "                        # print(token.text, \"about this much similar to\", wordOfInterest, \": \", wordOfInterest.similarity(token))\n",
    "        # Smoke test for the Counter(): \n",
    "        # for word, count in list(wordCounts.items())[:5]:\n",
    "        #    print(f\"'{word}': {count}\")\n",
    "        print(f'This is a dictionary of words most similar to the word \"{wordOfInterest.text}\" in \"{file}\".')\n",
    "        for word, similarity in highSimilarityDict.items():\n",
    "            count = wordCounts[word.text]\n",
    "            print(f\"{word}: similarity={similarity:.3f}, count={count}\")\n",
    "            # The `:3f` above basically just reduces the decimal places to three (as in 3 floating points). \n",
    "            # print(f\"{word}: similarity={similarity}, count={count}\")\n",
    "        print('\\n')\n",
    " \n",
    "\n",
    "for file in os.listdir(collection):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filepath = f\"{collection}/{file}\"\n",
    "        readTextFiles(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedac28-ea2a-41af-9044-5286b10a212e",
   "metadata": {},
   "source": [
    "## Your Turn! \n",
    "\n",
    "Adapt this code in your own Python file or notebook to explore similarity values based on your own collection of text files. Is this of interest in your project?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
