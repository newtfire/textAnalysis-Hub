{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43490178-1646-4cba-9119-7b7e724a68c4",
   "metadata": {},
   "source": [
    "# Reading File Directories and Exploring WordNet\n",
    "\n",
    "This notebook provides some guidance on working with file directories for a text corpus. It's also introducing some things we can explore with the WordNet dictionary accessible through NLTK. \n",
    "\n",
    "Read about [Wordnet in Chapter 2, section 5.2](https://www.nltk.org/book/ch02.html#wordnet) of the NLTK book. \n",
    "Credits: Some of our explanations are distilled from [David Birnbaum's introductory Wordnet notebook](https://github.com/djbpitt/wordnet/blob/master/Wordnet.ipynb) that we used to explore ambiguity of spooky words in projects some years ago! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388f1b3-a67c-478d-8acb-dd75d518e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173c67c-f20d-4f9a-b92a-2e4398a767b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOKE TEST: Explore Wordnet for specific words.\n",
    "wn.synsets('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61622953-4ec6-44fc-abda-061246343662",
   "metadata": {},
   "source": [
    "### Look at some synset data from WordNet\n",
    "Choose one of the synsets by its identifier, and lets explore what you can see with it.\n",
    "In the next couple of cells we explore how WordNet shares information: \n",
    "\n",
    "Wordnet shows:\n",
    "* A representative word that stands for set of meanings (a \"synset\"). There may be other representative words, and that same representative word might be used in different senses (and have multiple synsets).  \n",
    "* A part of speech (POS) identifier, like “n” for ‘noun’ or “v” for ‘verb’.\n",
    "* A two-digit number that distinguishes different synsets that may have the same head word and the same POS, but that convey different meaning. For example, the synsets 'ghost.n.01' and 'ghost.n.02' are two different nominal meanings that can be expressed by the lexeme “ghost.”\n",
    "\n",
    "In WordNet you can request:\n",
    "* the available **lemmas** which mean \"lexemes\" or the available set of words associated with a particular synset.\n",
    "* the **definition** associated with a synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493873cf-a9fb-42b2-8184-8eb5bbbabe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('net.v.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39aed6-7598-415d-b8f5-ede96f6bb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every lem in one of the synsets, isolate just the names without the extra stuff:\n",
    "for lem in wn.synset('net.v.02').lemmas():\n",
    "    print(lem.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947f920-158f-4faf-9e8a-129cdac58235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll just return the count of the available lemmas for each synset for \"clear\"\n",
    "# In this output we're just surveying the data as lists:\n",
    "for synset in wn.synsets('clear'):\n",
    "    print(synset, \": \", synset.lemma_names(), \": \", len(synset.lemma_names()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bfaa4-e161-42c3-bee0-286030890e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for the definitions and part of speech available for a word lemma names in WordNet\n",
    "for synset in wn.synsets('clear'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750f700-bf75-4cb6-892e-453df3d3deab",
   "metadata": {},
   "source": [
    "## Ambiguity from WordNet's point of view\n",
    "When words are ambiguous, they can multiple shades of meaning or different ways we could interpret them. How we decide on the meaning depends on the context of the words. WordNet can't tell you everything about how you could interpret every word in your project, but when words are available in its vast database, you can explore how many synsets (or possible meaningful interpretations) it has on file for that word. You can find which words depend the most on specific contexts for their meaning. \n",
    "\n",
    "To find out WordNet's sense of how ambiguous a word is, you could just count the number of available synsets for it with Python's **len()**.\n",
    "Note: when you want ALL the synsets, use **wn.synsets** (plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0afc7c-31ac-462a-b88c-8ebb841b01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_words = ['creep', 'eldritch', 'horror', 'cry', 'scream', 'ghost', 'scare']\n",
    "for w in spooky_words:\n",
    "    synsets = len(wn.synsets(w))\n",
    "    print(\"The word \", w, \"belongs to \", synsets, \"synsets in WordNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd59ab9-4763-4ca0-9ae3-bb5d38fe994f",
   "metadata": {},
   "source": [
    "### WordNet's morphy: for inflected forms like plural vs. singular\n",
    "Wordnet won't have an entry for plural forms of words (for example). For other things like -ing words, those tend to have their own WordNet entry. \n",
    "\n",
    "When working with project data, **run your word tokens through Wordnet's morphy in case they need to be looked up in a lemmatized form**\n",
    "`wn.morphy('ghosts')`. Try it out on various word forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10215ca7-f49c-4160-bd81-393c6890c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.morphy('cries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbb99d-6677-4c2b-a77b-b0520c5437f5",
   "metadata": {},
   "source": [
    "### A Workflow for projects exploring ambiguity\n",
    "* Start by isolating a set of words of interest from your project. (One idea: Use nltk's pos tagger to identify all the verbs in your files and retrieve a set of them).\n",
    "* Deliver them to WordNet to retrieve their synset counts\n",
    "* Output the information on each distinct word from Python to a simple XML, TSV, or JSON (We'll probably use XML)\n",
    "* Options:\n",
    "    * Use the frequency distribution plotting in Python to see how frequently ambiguous words are used in your corpus (plotting is a bit limited)\n",
    "    * Write XSLT to map that information back into your XML files: We can use this to **instantiate** the uses of ambiguous words: retrieve counts of how frequently they appear in the text, and plot with SVGs of your own design. (You could make an SVG representing info about the most ambiguous words. You could also make specific SVGs for ambiguous words of interest to show how they're distributed throughout your texts... what would be interesting to visualize?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5b503-aa93-4348-a4f5-b096528a5d61",
   "metadata": {},
   "source": [
    "## Part of Speech (and other related kinds of) Tagging\n",
    "NLTK (and spacY and other language models) offer Part of Speech tagging. We could use this to collect sets of distinct verbs, nouns, adjectives. \n",
    "* We could learn more about the words in a set by sending them to WordNet.\n",
    "* We could also look at words as a network: How often do specific characters rely on a word of interest? Which characters share a word of interest? Or which words of interest are shared by say, male vs. female characters?  (Here's [a student project](http://bamfs.obdurodon.org/allFilms_Results.xhtml) that explored a collection of colorful swear words used by characters in Quentin Tarantino's films.)\n",
    "\n",
    "In our Text Analysis class, you didn't spend time marking specific words, but you could do that using regex search and replace over your texts. But you could also get some help from POS tagging from NLTK, and mapping that information back to your XML files for further analysis. So let's explore that.  We'll look at POS (part of speech) tagging, but note that it's related to Named Entity Recognition and sentiment analysis. \n",
    "\n",
    "(Full disclosure: We like POS for reasons of interesting patterns it can display without overdetermining what the data means in context. You can show interesting patterns, especially by working with POS tagging together with WordNet analyses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b699d-a22f-42bb-9eee-dea1add1f4e8",
   "metadata": {},
   "source": [
    "## How to get NLTK to do POS tagging\n",
    "* Let's open a file, tokenize it, prepare it for nltk to analyze.\n",
    "* Then apply the .pos tagger\n",
    "* What we see in the output is a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce8f1a-8b4e-4c21-a53c-e2e23fe8d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "filepath = 'hughes-txt/sixteen.txt'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# Make a list of tokens in your text. \n",
    "# tokenList = f.split()\n",
    "tokenList = word_tokenize(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c80516-cd28-4b7f-adc6-2572a39726a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the complexity by: 1) lowercasing them, and 2) returning the set() of words (remove multiple of the same value)\n",
    "lowercaseTokens = [token.lower() for token in tokenList]\n",
    "uniqueTokens = set(lowercaseTokens)\n",
    "print(uniqueTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d6fd3-e2c8-4dd8-a1d0-2a5f397c68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out the NLTK POS tagger on the uniqueTokens\n",
    "pos_tag(uniqueTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93758d5-09c2-46e3-b075-f6bdde741c0c",
   "metadata": {},
   "source": [
    "### Wait...what is that POS category?\n",
    "Find out what a category is by asking NLTK:\n",
    "`nltk.help.upenn_tagset('VBZ')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fa3b9-2c2e-4515-833c-10cb26345da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('VBD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405aeaaa-c8d6-46d8-b7e7-208e7c23968a",
   "metadata": {},
   "source": [
    "We can limit our list of words of interest by asking for words of a particular kind, isolating them by POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232cb7a-8793-4034-a6b0-d194e5a7d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POStagged = pos_tag(uniqueTokens)\n",
    "tagsIwant = ['VBD', 'VBN', 'VBZ']\n",
    "# This is a Python list comprehension that'll help us with our list of tuples [(word, tag), (word, tag), ...]\n",
    "shortList = [word for word, tag in POStagged if tag in tagsIwant]\n",
    "print(len(shortList))\n",
    "print(shortList)\n",
    "\n",
    "# Fancy string formatting (not super useful here, just reviewing it): \n",
    "# f'My short list is {shortList} and it is this long: {len(shortList)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f37dba-dc58-487c-ad4b-eb169f1e9155",
   "metadata": {},
   "source": [
    "## Send them to WordNet for synset lookup\n",
    "\n",
    "* Use list comprehension\n",
    "* Use wn.morphy to simplify the word to its synset lemma form\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a55e05-77a5-45c9-8add-f2094314c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in shortList:\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    # lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    if synsets:\n",
    "        print(f\" Word: {w}, Number of Synsets (Ambiguity): {len(synsets)}\") \n",
    "              \n",
    "        \n",
    "        # for syn in synsets:\n",
    "        #   print(f\"  Synset: {syn.name()}, Definition: {syn.definition()}}\")\n",
    "    \n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd9132-801c-415e-bf59-ed9712a02911",
   "metadata": {},
   "source": [
    "## Read in just one file from a directory in your repo\n",
    "Open one of your text files in your repo for reading.\n",
    "In this example, we'll climb directories, and that means we'll use the os library to show you how to handle filepaths.\n",
    "\n",
    "When your file isn't immediately in the same folder as your Python script, and you need to climb for it, start with os library by getting the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e348fd7-3de0-40aa-a76f-688d195775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd is my shorthand for \"current working directory\"\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb7420-b1d2-487d-9f5e-372a218afc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# climb up one directory and retrieve a file. (here's how you would do that)\n",
    "# (ADAPT THIS CODE TO REACH DOWN OR UP AS NEEDED.)\n",
    "filepath = '../grimm.txt'\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc90f2c-7478-49d2-953d-f94c3b994f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Python must OPEN and READ the text file in order to process it:\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# readFile = f.read()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432701a-b8d5-4ff0-9be6-3f3002461a14",
   "metadata": {},
   "source": [
    "## Read in some project data from a collection of text files\n",
    "You have a file directory with some text files probably near your Python script. \n",
    "\n",
    " From the \"for loop\" in the next cell, we can then write code to process information about each file separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d51112-96be-4409-884c-aa77853b81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, we defined cwd as our current working directory holding this file.\n",
    "# list directories:\n",
    "os.listdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf29225-9f02-4b60-b4ee-231302ba4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = os.path.join(cwd, 'hughes-txt')\n",
    "os.listdir(coll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe0019-0692-4aed-84f9-ec4ca6f3a69b",
   "metadata": {},
   "source": [
    "## Processing the directory as one corpus\n",
    "What if we want to create an NLTK corpus of these texts and process them as one unit? See the [NLTK book chapter 2 section 1.9](https://www.nltk.org/book/ch02.html#loading-your-own-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9866da8-3164-4c55-b6cd-c8028ce33dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(coll):\n",
    "   if file.endswith(\".txt\"):\n",
    "        filepath = f\"{coll}/{file}\"\n",
    "        print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78248ca-250a-4253-9964-209e01f7d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'hughes-txt'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "corpus.fileids()\n",
    "# Check on one file in the collection\n",
    "# corpus.words('breakfast.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e95de5-5a29-4baa-84c9-23cd49eafe49",
   "metadata": {},
   "source": [
    "## YOUR TURN!\n",
    "What do you need to do to process texts in your directory with NLTK tools?\n",
    "You will need to:\n",
    "* Turn your text(s) into a list of tokens!\n",
    "* Then you can process those tokens with NLTK\n",
    "\n",
    "### Your assignment is...\n",
    "Split up your texts into a list of tokens, and do some new NLTK processing of them.\n",
    "Look Stuff Up: See if you can use NLTK to output a **set** of a certain kind of word: could be...\n",
    "* **part of speech** (find out how NLTK can retrieve pos (part-of-speech) information). Retrieve pos information!\n",
    "* Pull a **set** of all the tokens that share a specific part of speech. Make a set() because it removes multiple instances: (it's the same as taking distinct-values() in XPath. (Want all the adjectives? All the verbs? etc.)\n",
    "* Do something interesting with NTLK over that set of words. Try out wordnet synsets, or experiment with frequency distributions, or something else that looks nifty in NLTK. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
