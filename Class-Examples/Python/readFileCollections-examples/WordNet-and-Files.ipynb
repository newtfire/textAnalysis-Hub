{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43490178-1646-4cba-9119-7b7e724a68c4",
   "metadata": {},
   "source": [
    "# Reading File Directories and Exploring WordNet\n",
    "\n",
    "This notebook provides some guidance on working with file directories for a text corpus. It's also introducing some things we can explore with the WordNet dictionary accessible through NLTK.\n",
    "\n",
    "Read about [Wordnet in Chapter 2, section 5.2](https://www.nltk.org/book/ch02.html#wordnet) of the NLTK book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388f1b3-a67c-478d-8acb-dd75d518e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173c67c-f20d-4f9a-b92a-2e4398a767b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOKE TEST: Explore Wordnet for specific words.\n",
    "wn.synsets('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61622953-4ec6-44fc-abda-061246343662",
   "metadata": {},
   "source": [
    "### Look at some synset data from WordNet\n",
    "Choose one of the synsets by its identifier, and lets explore what you can see with it.\n",
    "In the next couple of cells we explore the various lemmas connected to a particular synset for a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7cb91-26f9-4a68-9471-b2d883adcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('net.v.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b6b62-a4da-4757-a9b2-179cfbadb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('clear'):\n",
    "    print(synset.lemma_names(), len(synset.lemma_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd9132-801c-415e-bf59-ed9712a02911",
   "metadata": {},
   "source": [
    "## Read in just one file from a directory in your repo\n",
    "Open one of your text files in your repo for reading.\n",
    "In this example, we'll climb directories, and that means we'll use the os library to show you how to handle filepaths.\n",
    "\n",
    "When your file isn't immediately in the same folder as your Python script, and you need to climb for it, start with os library by getting the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e348fd7-3de0-40aa-a76f-688d195775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd is my shorthand for \"current working directory\"\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb7420-b1d2-487d-9f5e-372a218afc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# climb up one directory and retrieve a file. (here's how you would do that)\n",
    "# (ADAPT THIS CODE TO REACH DOWN OR UP AS NEEDED.)\n",
    "filepath = '../grimm.txt'\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc90f2c-7478-49d2-953d-f94c3b994f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Python must OPEN and READ the text file in order to process it:\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# readFile = f.read()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432701a-b8d5-4ff0-9be6-3f3002461a14",
   "metadata": {},
   "source": [
    "## Read in some project data from a collection of text files\n",
    "You have a file directory with some text files probably near your Python script. \n",
    "\n",
    " From the \"for loop\" in the next cell, we can then write code to process information about each file separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d51112-96be-4409-884c-aa77853b81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, we defined cwd as our current working directory holding this file.\n",
    "# list directories:\n",
    "os.listdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf29225-9f02-4b60-b4ee-231302ba4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = os.path.join(cwd, 'hughes-txt')\n",
    "os.listdir(coll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe0019-0692-4aed-84f9-ec4ca6f3a69b",
   "metadata": {},
   "source": [
    "## Processing the directory as one corpus\n",
    "What if we want to create an NLTK corpus of these texts and process them as one unit? See the [NLTK book chapter 2 section 1.9](https://www.nltk.org/book/ch02.html#loading-your-own-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9866da8-3164-4c55-b6cd-c8028ce33dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(coll):\n",
    "   if file.endswith(\".txt\"):\n",
    "        filepath = f\"{coll}/{file}\"\n",
    "        print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78248ca-250a-4253-9964-209e01f7d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'hughes-txt'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "corpus.fileids()\n",
    "# Check on one file in the collection\n",
    "# corpus.words('breakfast.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e95de5-5a29-4baa-84c9-23cd49eafe49",
   "metadata": {},
   "source": [
    "## YOUR TURN!\n",
    "What do you need to do to process texts in your directory with NLTK tools?\n",
    "You will need to:\n",
    "* Turn your text(s) into a list of tokens!\n",
    "* Then you can process those tokens with NLTK\n",
    "\n",
    "### Your assignment is...\n",
    "Split up your texts into a list of tokens, and do some new NLTK processing of them.\n",
    "Look Stuff Up: See if you can use NLTK to output a **set** of a certain kind of word: could be...\n",
    "* **part of speech** (find out how NLTK can retrieve pos (part-of-speech) information). Retrieve pos information!\n",
    "* Pull a **set** of all the tokens that share a specific part of speech. Make a set() because it removes multiple instances: (it's the same as taking distinct-values() in XPath. (Want all the adjectives? All the verbs? etc.)\n",
    "* Do something interesting with NTLK over that set of words. Try out wordnet synsets, or experiment with frequency distributions, or something else that looks nifty in NLTK. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d506ab-889f-450b-aa31-70e164d08c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
